{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential Privacy in Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef56gCUqrdVn"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RseeuA7veIHU"
      },
      "source": [
        "!pip install tensorflow_privacy\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ML23FlueTr"
      },
      "source": [
        "train, test = tf.keras.datasets.cifar10.load_data()\n",
        "train_data, train_labels = train\n",
        "test_data, test_labels = test\n",
        "\n",
        "train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "train_data = train_data.reshape(train_data.shape[0], 32, 32, 3)\n",
        "test_data = test_data.reshape(test_data.shape[0], 32, 32, 3)\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVDcswOCtlr3"
      },
      "source": [
        "## Define and tune learning model hyperparameters\n",
        "Set learning model hyperparamter values. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E14tL1vUuTRV"
      },
      "source": [
        "epochs = 15\n",
        "batch_size = 80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVw_r2Mq7ntd"
      },
      "source": [
        "l2_target = np.arange(1.6, 2.5, 0.1).tolist()\n",
        "noise_target = np.arange(1.2, 2.1, 0.1).tolist()\n",
        "noise_default = [1.1 for _ in range(len(l2_target))]\n",
        "l2_default = [1.5 for _ in range(len(noise_target))]\n",
        "\n",
        "\n",
        "l2_norm_clip = l2_target + l2_default\n",
        "noise_multiplier = noise_default + noise_target\n",
        "num_microbatches = 80\n",
        "learning_rate = 0.15\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAmHcNOmHc5"
      },
      "source": [
        "## Build the learning model\n",
        "\n",
        "Define a convolutional neural network as the learning model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCOo8aOLmFta"
      },
      "source": [
        "test_acc_hist = []\n",
        "eps_hist = []\n",
        "rdp_hist = []\n",
        "\n",
        "for i, (l2_clip, noise) in enumerate(zip(l2_norm_clip, noise_multiplier)):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, 8,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu',\n",
        "                            input_shape=(32, 32, 3)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(32, 4,\n",
        "                            strides=2,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_clip,\n",
        "      noise_multiplier=noise,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  model.fit(train_data, train_labels,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_labels),\n",
        "            batch_size=batch_size)\n",
        "  \n",
        "  # CALC EPSILON\n",
        "  eps, rdp = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
        "      n=train_data.shape[0], \n",
        "      batch_size=batch_size, \n",
        "      noise_multiplier=noise, \n",
        "      epochs=epochs, \n",
        "      delta=1 / train_data.shape[0]\n",
        "      )\n",
        "  eps_hist.append(eps), rdp_hist.append(rdp)\n",
        "  \n",
        "  # CALC ACCURACY\n",
        "  score, acc = model.evaluate(test_data, test_labels,\n",
        "                              batch_size=batch_size)\n",
        "  test_acc_hist.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3ecgBN3YZUj"
      },
      "source": [
        "# MOBILENET\n",
        "TRAIN MOBILENET HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvReJLNGXiAc"
      },
      "source": [
        "test_acc_hist = []\n",
        "eps_hist = []\n",
        "rdp_hist = []\n",
        "\n",
        "for i, (l2_clip, noise) in enumerate(zip(l2_norm_clip, noise_multiplier)):\n",
        "  model = tf.keras.applications.mobilenet.MobileNet(\n",
        "    input_shape=(32, 32, 3), \n",
        "    alpha=1.0, \n",
        "    depth_multiplier=1, \n",
        "    dropout=1e-3, \n",
        "    include_top=True, \n",
        "    weights=None, \n",
        "    input_tensor=None, \n",
        "    pooling=None, \n",
        "    classes=10\n",
        "  )\n",
        "\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_clip,\n",
        "      noise_multiplier=noise,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  model.fit(train_data, train_labels,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_labels),\n",
        "            batch_size=batch_size)\n",
        "  \n",
        "  # CALC EPSILON\n",
        "  eps, rdp = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
        "      n=train_data.shape[0], \n",
        "      batch_size=batch_size, \n",
        "      noise_multiplier=noise, \n",
        "      epochs=epochs, \n",
        "      delta=1 / train_data.shape[0]\n",
        "      )\n",
        "  eps_hist.append(eps), rdp_hist.append(rdp)\n",
        "  \n",
        "  # CALC ACCURACY\n",
        "  score, acc = model.evaluate(test_data, test_labels,\n",
        "                              batch_size=batch_size)\n",
        "  test_acc_hist.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JriewGVlUUho"
      },
      "source": [
        "# RESNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2H6fd2tUHxW"
      },
      "source": [
        "test_acc_hist = []\n",
        "eps_hist = []\n",
        "rdp_hist = []\n",
        "\n",
        "for i, (l2_clip, noise) in enumerate(zip(l2_norm_clip, noise_multiplier)):\n",
        "  model = tf.keras.applications.resnet50.ResNet50(\n",
        "    include_top=True, \n",
        "    weights=None, \n",
        "    input_tensor=None, \n",
        "    input_shape=(32, 32, 3),\n",
        "    pooling=None, classes=10\n",
        "    )\n",
        "\n",
        "  optimizer = DPGradientDescentGaussianOptimizer(\n",
        "      l2_norm_clip=l2_clip,\n",
        "      noise_multiplier=noise,\n",
        "      num_microbatches=num_microbatches,\n",
        "      learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  model.fit(train_data, train_labels,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_labels),\n",
        "            batch_size=batch_size)\n",
        "  \n",
        "  # CALC EPSILON\n",
        "  eps, rdp = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
        "      n=train_data.shape[0], \n",
        "      batch_size=batch_size, \n",
        "      noise_multiplier=noise, \n",
        "      epochs=epochs, \n",
        "      delta=1 / train_data.shape[0]\n",
        "      )\n",
        "  eps_hist.append(eps), rdp_hist.append(rdp)\n",
        "  \n",
        "  # CALC ACCURACY\n",
        "  score, acc = model.evaluate(test_data, test_labels,\n",
        "                              batch_size=batch_size)\n",
        "  test_acc_hist.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
